# These are the default settings
# - All settings are defined here
# - The types used in this file are enforced

_description: Default settings

# When enabled, attachments found in chat messages are automatically sent to the LLM.
# In cases where the syntax is creating false positives, this provides an easy way to turn it off.
chat.enable_attachments: true
# Show the bottom toolbar in the chat interface
chat.enable_toolbar: true
# The history file to use in the chat interface for readline history.
# When set to an empty string, there is no history file
chat.history_file: '~/.lair/history'
# When displaying fields in the toolbar, use this style for enabled flags
chat.flag_off_style: 'bg:#006f6f'
# When displaying fields in the toolbar, use this style for disabled flags
chat.flag_on_style: 'bg:#00f1f1 bold'
# When enabled, pressing enter will move to the new line instead of submitting. To submit, us M-RET (escape enter.)
chat.multiline_input: false
# The prompt to use in the chat interface. This is a python f-string
# Provided variables: flags, mode, model
chat.prompt_template: '<style fg="#8f8f8f">{mode}</style>> '
# Toolbar style when a notification is being shown
chat.toolbar_flash_style: 'fg:#101080 bg:#f1f1f1'
# The default toolbar style
chat.toolbar_style: 'fg:#1f1f1f bg:#f1f1f1'
# This toolbar template to use. This is a python f-string and should not have any newlines.
# The YAML ">-" keeps it folded so that there are no newlines.
# Provided variables: flags, mode, model
chat.toolbar_template: >-
  [{flags}]
  <style bg="#6f6f6f">mode</style> <b>{mode}</b> 
  <style bg="#6f6f6f">model</style> <b>{model}</b>
# The Default toolbar text style
chat.toolbar_text_style: 'fg:#1f1f1f bg:#f1f1f1'

# The address of the Comfy UI API to use
comfy.url: http://127.0.0.1:8188
# If using comfy over HTTPS, this allows for disabling certificate verification
comfy.verify_ssl: true

comfy.image.batch_size: 1
comfy.image.cfg: 8.0
comfy.image.denoise: 1.0
comfy.image.loras: __null_str
comfy.image.model_name: v1-5-pruned-emaonly.safetensors
comfy.image.negative_prompt: ''
# When writing out files the output_file is used as is when there is a single output.  If there are multiple outputs the
# basename becomes a prefix and a zero-padded counter is added. (For example output000000.png)
comfy.image.output_file: 'output.png'
comfy.image.output_height: 512
comfy.image.output_width: 512
comfy.image.prompt: ''
comfy.image.sampler: 'euler'
comfy.image.scheduler: 'normal'
comfy.image.seed: __null_int
comfy.image.steps: 20

# ltxv_i2v: LTXV Image to Video
#   Based on https://github.com/Lightricks/LTX-Video
# If a prompt isn't provided via comfy.ltxv_i2v.prompt, then one is automatically generated. The Florence model takes an
# image and generates a prompt for it. It's generated prompt is followed by the "extra" value, which probably should end
# in a period.  That is then followed by the suffix. This style is taken from the LTXV Image to Video Comfy workflow.
comfy.ltxv_i2v.auto_prompt_extra: ''
comfy.ltxv_i2v.auto_prompt_suffix: ' The scene is captured in real-life footage.'
comfy.ltxv_i2v.batch_size: 1
comfy.ltxv_i2v.cfg: 3.0
comfy.ltxv_i2v.clip_name: 't5xxl_fp16.safetensors'
comfy.ltxv_i2v.denoise: 1.0
comfy.ltxv_i2v.florence_model_name: 'microsoft/Florence-2-base'
# The seed for the florence model to use. If set to null, it will be randomly selected each cycle. Setting to null
# effectively disables caching of Florence results.
comfy.ltxv_i2v.florence_seed: 42
comfy.ltxv_i2v.frame_rate: 25
comfy.ltxv_i2v.image_resize_height: 800
comfy.ltxv_i2v.image_resize_width: 800
comfy.ltxv_i2v.model_name: 'ltx-video-2b-v0.9.1.safetensors'
comfy.ltxv_i2v.negative_prompt: 'worst quality, inconsistent motion, blurry, jittery, distorted, watermarks'
# Number of frames to generate  (Must be N * 8 + 1)
comfy.ltxv_i2v.num_frames: 105
# When writing out files the output_file is used as is when there is a single output.  If there are multiple outputs the
# basename becomes a prefix and a zero-padded counter is added. (For example output000000.mp4)
comfy.ltxv_i2v.output_file: 'output.mp4'
# Format to generate an output file in. This is from VHS Video Combine.
# Examples: image/gif, image/webp, video/h264-mp4, video/webm
comfy.ltxv_i2v.output_format: 'video/h264-mp4'
# When true, VHS Video Combine exports the video followed by it in reverse to create a loop
comfy.ltxv_i2v.pingpong: false
# When a prompt is provided this exact prompt is used instead of the auto-generated prompt.
# Auto prompt and florence settings are ignored when this is used.
comfy.ltxv_i2v.prompt: null
comfy.ltxv_i2v.sampler: 'euler_ancestral'
comfy.ltxv_i2v.scheduler: 'normal'
comfy.ltxv_i2v.seed: null
comfy.ltxv_i2v.steps: 25
comfy.ltxv_i2v.stg: 1.0
# This is the block_indices parameter from the LTXV Apply STG node
# The Comfy LTXV I2V workflow recommends 11, 14, and 19.
# The value is a string and can contain multiple comma delimited items.
comfy.ltxv_i2v.stg_block_indices: '14'
comfy.ltxv_i2v.stg_rescale: 0.75

comfy.ltxv_prompt.auto_prompt_extra: ''
comfy.ltxv_prompt.auto_prompt_suffix: ' The scene is captured in real-life footage.'
comfy.ltxv_prompt.florence_model_name: 'microsoft/Florence-2-base'
comfy.ltxv_prompt.florence_seed: null
comfy.ltxv_prompt.image_resize_height: 800
comfy.ltxv_prompt.image_resize_width: 800
comfy.ltxv_prompt.output_file: 'output.txt'

# These are from the original lair. The open source lair currently has
# no database and so these have no effect.
# NOTE: These are only here as convenience for me, since I use both the old and new lair. -dhaber
database.host: 'localhost'
database.host_environment_variable: 'DB_HOST'
database.name: 'lair'
database.name_environment_variable: 'DB_NAME'
# WARNING: database.password is discouraged except for local test
# All settings are stored in saved session files, including passwords.
database.password: 'lair'
database.password_environment_variable: 'DB_PASS'
database.port: 5432
database.port_environment_variable: 'DB_PORT'
database.user: 'lair'
database.user_environment_variable: 'DB_USER'

# This used to toggle langchain verbose mode.
# It currently might not do anything, and will likely be removed in the future.
debug.verbose: false

# The max_completion_tokens to set in the API request, for endpoints that honor this
model.max_tokens: __null_int
# The name of the model to run, used by the API
model.name: ''
# The temperature to set in the API request
model.temperature: __null_float
# When enabled and attachments are used, the filenames are provided to the model.
model.provide_attachment_filenames: false

# The base URL to use when talking to OpenAI compatible APIs.  Set as null
# for the default official OpenAI URL. For ollama, use something like http://localhost:11434/v1
openai.api_base: __null_str
# Environment variable name containing API key to use against OpenAI compatible APIs.
openai.api_key_environment_variable: 'OPENAI_API_KEY'
openai.max_retries: 2,
openai.timeout: 65,

# How many entries to store in the history file.
session.max_history_length: __null_int
# The type of session to use
# Currently, open openai_chat is supported.
#   openai_chat: Use an OpenAI Compatible API
session.type: 'openai_chat'

style.error: 'red'
style.human_output: ''
style.human_output_heading: 'bold white'
style.llm_output: 'dim cyan'
style.llm_output_heading: 'bold cyan'
# When showing responses, this determines if they are rendered as markdown or shown as text
style.render_markdown: true
# When displaying errors, this determines whether rich tracebacks or standard ones should be shown
style.render_rich_tracebacks: false
style.system_message: 'dim magenta'
style.system_message_heading: 'bold magenta'
style.tool_message: 'dim green'
style.tool_message_heading: 'bold green'
style.user_error: 'dim red'
# Whether or not output should word-wrap, or be written as-is
style.word_wrap: true
